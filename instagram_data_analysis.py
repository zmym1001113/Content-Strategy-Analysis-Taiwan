# -*- coding: utf-8 -*-
"""Instagram Data Analysis

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1roetphoo9pDg-3Ucg-WPiqLtQFWcTy7W

# Instagram Data Analysis

## 1 Read File from G sheet

df is the main dataframe, 5 rows √ó 22 columns, N=107


> ['Post ID', 'Account ID', 'Account username', 'Account name', 'Description', 'Description (English)', 'ÂàÜÈ°ûÁ∑®Á¢º', 'Duration (sec)', 'Publish time', 'Permalink', 'Post type', 'Data comment', 'Date', 'Total_interactions', 'Avg_eng', 'Views', 'Reach', 'Likes', 'Shares', 'Follows', 'Comments', 'Saves']
"""

import pandas as pd

# Step 1: Install gspread (optional, usually preinstalled in Colab)
!pip install --quiet gspread

# Step 2: Import Google authentication
from google.colab import auth
auth.authenticate_user()

# Step 3: Connect to Google Drive
from google.auth import default
import gspread
creds, _ = default()
gc = gspread.authorize(creds)

# Step 4: Open your sheet (replace with the actual sheet name)
sheet = gc.open("data").sheet1   # or gc.open_by_url("https://docs.google.com/spreadsheets/d/xxxxxx/edit")

# Step 5: Convert to DataFrame #the main dataframe call df
df = pd.DataFrame(sheet.get_all_records())
df.head()

#get the col names
print(df.columns.tolist())

"""## 2 Clean data

here we get the df_clean



> ['Description', 'Des_eng', 'Label', 'Duration (sec)', 'Publish time', 'Permalink', 'Post type', 'Date', 'Total_interactions', 'Avg_eng', 'Views', 'Reach', 'Likes', 'Shares', 'Follows', 'Comments', 'Saves']




"""

# Rename
df.rename(columns={'ÂàÜÈ°ûÁ∑®Á¢º': 'Label',
                   'Description (English)': 'Des_eng',
                   'Avg_eng' :'Engagement_Rate'}, inplace=True)
df.columns

# select only few col
df = df[df['Post type'] != 'IG image']

df_clean = df.drop(columns=['Post ID', 'Account ID', 'Account username', 'Account name', 'Data comment'], errors='ignore')


# get rid of IG image since it's only 1 post


df_clean.head()

#get the col names
print(df_clean.columns.tolist())

"""## 3 Explore the data (total_interaction)"""

# how many different type of post?
df['Post type'].unique()

#group by post type
#grouped_post_type = df.groupby('Post type')

#Ë®àÁÆóÁü≠ÂΩ±ÁâáÂíåË≤ºÊñáÁöÑÁ∏Ω‰∫íÂãïÊï∏
summary = df.groupby('Post type')['Total_interactions'].agg(['count','mean','std','median','min','max']).reset_index()
print(summary)

import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(8,5))
sns.histplot(data=df,
             x='Total_interactions',
             hue='Post type',
             kde=True,
             bins=30,
             alpha=0.6)

plt.title('Distribution of Total Interactions (Before Removing Outliers)')
plt.xlabel('Total Interactions')
plt.ylabel('Count')
plt.show()

# print out the table without removing outliers, currently, n=106
!pip install reportlab

from reportlab.lib import colors
from reportlab.lib.pagesizes import letter
from reportlab.platypus import SimpleDocTemplate, Table, TableStyle, Paragraph, Spacer
from reportlab.lib.styles import getSampleStyleSheet
import pandas as pd

# example summary
summary = pd.DataFrame({
    'Post type': ['IG carousel', 'IG reel'],
    'N': [27, 79],
    'Mean': [859.44, 980.22],
    'SD': [2146.25, 3849.87],
    'Median': [158.0, 100.0],
    'Min': [35, 23],
    'Max': [10870, 29165]
})

# convert to list for reportlab table
data = [summary.columns.tolist()] + summary.values.tolist()

# create PDF
pdf_path = "/content/drive/MyDrive/data/APA_Table.pdf"
doc = SimpleDocTemplate(pdf_path, pagesize=letter)
styles = getSampleStyleSheet()
elements = []

# caption
caption = Paragraph("Table 1\nDescriptive Statistics by Post Type", styles["Normal"])
elements.append(caption)
elements.append(Spacer(1, 12))

# table
table = Table(data, hAlign='CENTER')
table.setStyle(TableStyle([
    ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),
    ('ALIGN', (0, 0), (-1, -1), 'CENTER'),
    ('LINEBELOW', (0, 0), (-1, 0), 1, colors.black),
    ('LINEABOVE', (0, -1), (-1, -1), 1, colors.black),
    ('BOTTOMPADDING', (0, 0), (-1, 0), 6),
    ('TOPPADDING', (0, 0), (-1, 0), 6),
    ('FONTSIZE', (0, 0), (-1, -1), 10),
]))

elements.append(table)
doc.build(elements)

print(f"‚úÖ APA table exported successfully: {pdf_path}")

"""## 4 Remove the total interaction Outleirs  (total_interaction)"""

# Calculate IQR for each post type separately
def remove_outliers(group):
    Q1 = group['Total_interactions'].quantile(0.25)
    Q3 = group['Total_interactions'].quantile(0.75)
    IQR = Q3 - Q1
    lower = Q1 - 1.5 * IQR
    upper = Q3 + 1.5 * IQR
    return group[(group['Total_interactions'] >= lower) & (group['Total_interactions'] <= upper)]

df_no_outliers = df.groupby('Post type', group_keys=False).apply(remove_outliers)

# check how many removed
print("Before:", len(df))
print("After:", len(df_no_outliers))

# who are the outlier?

def get_outliers(group):
    Q1 = group['Total_interactions'].quantile(0.25)
    Q3 = group['Total_interactions'].quantile(0.75)
    IQR = Q3 - Q1
    lower = Q1 - 1.5 * IQR
    upper = Q3 + 1.5 * IQR
    return group[(group['Total_interactions'] < lower) | (group['Total_interactions'] > upper)]

outliers = df.groupby('Post type', group_keys=False).apply(get_outliers)

print("Number of outliers:", len(outliers))
outliers[['Post type', 'Total_interactions', 'Description (English)', 'Permalink']]

#1Ô∏è‚É£ Recalculate descriptive stats
#Run your groupby summary again:

summary_clean = df_no_outliers.groupby('Post type')['Total_interactions'].agg(['count','mean','std','median','min','max']).reset_index()
print(summary_clean)

#distribution visual

import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(8,5))
sns.histplot(data=df_no_outliers,
             x='Total_interactions',
             hue='Post type',
             kde=True,
             bins=30,
             alpha=0.6)

plt.title('Distribution of Total Interactions (After Removing Outliers)')
plt.xlabel('Total Interactions')
plt.ylabel('Count')
plt.show()

#2Ô∏è‚É£ Check normality (optional but useful)
#If you plan to do statistical tests:

from scipy.stats import shapiro
for t, g in df_no_outliers.groupby('Post type'):
    print(t, shapiro(g['Total_interactions']))

"""```
IG carousel ShapiroResult(statistic=np.float64(0.8244585346555124), pvalue=np.float64(0.0009694420065747539))
IG reel ShapiroResult(statistic=np.float64(0.8290558851640499), pvalue=np.float64(1.5448489791880302e-07))
```

> üß† Shapiro‚ÄìWilk test checks if your data are normally distributed.

Null hypothesis (H‚ÇÄ): the data are normal.

Alternative hypothesis (H‚ÇÅ): the data are not normal.

for your results:

IG carousel: p = 0.00097

IG reel: p = 0.00000015

since both p-values are < .05, you reject H‚ÇÄ, meaning the distributions of total interactions for both post types are not normal.

üëâ In short: your engagement data are skewed, probably because a few posts performed extremely well (even after removing outliers).
so for comparing groups, you should use a non-parametric test like Mann‚ÄìWhitney U test instead of a t-test.
"""

from scipy.stats import mannwhitneyu

# Split by post type
carousel = df_no_outliers[df_no_outliers['Post type'] == 'IG carousel']['Total_interactions']
reel = df_no_outliers[df_no_outliers['Post type'] == 'IG reel']['Total_interactions']

# Run Mann‚ÄìWhitney U test (two-tailed)
stat, p = mannwhitneyu(carousel, reel, alternative='two-sided')

print(f"U statistic = {stat:.2f}")
print(f"p-value = {p:.4f}")

"""Interpretation:

If p < .05, there‚Äôs a significant difference in engagement between IG carousels and reels.

If p ‚â• .05, no significant difference is found.
p = 0.0634 is greater than .05, so the difference in total interactions between IG carousels and IG reels is not statistically significant.

In plain terms: even though reels seem to get higher average engagement, the variation is large enough that it‚Äôs not a consistent or reliable difference across your sample.

üìÑ APA-style report example:

A Mann‚ÄìWhitney U test indicated that total interactions did not significantly differ between IG carousels (Mdn = 158) and IG reels (Mdn = 100), U = 1014.00, p = .063.



> since p = 0.063 is above .05, it means there‚Äôs no statistically significant difference ‚Äî so, yeah, carousel and reel posts perform roughly the same in your sample.

> but just a nuance üëá

> it doesn‚Äôt mean they‚Äôre identical, just that the difference isn‚Äôt strong or consistent enough to rule out random chance.

> you can cautiously say: ‚ÄúPost type does not appear to significantly influence engagement.‚Äù

> still, if you see patterns (like reels having higher mean but big variability), that could suggest some practical difference, even if not statistically significant yet.


"""

# see the effect size, how meaningful it is for this test, unable to prove H0 wrong, to be trully meaningful?
from scipy.stats import mannwhitneyu, norm
import numpy as np

# split groups again
carousel = df_no_outliers[df_no_outliers['Post type'] == 'IG carousel']['Total_interactions']
reel = df_no_outliers[df_no_outliers['Post type'] == 'IG reel']['Total_interactions']

# Mann‚ÄìWhitney U test
U, p = mannwhitneyu(carousel, reel, alternative='two-sided')

# calculate Z and r
n1, n2 = len(carousel), len(reel)
mean_U = n1 * n2 / 2
std_U = np.sqrt(n1 * n2 * (n1 + n2 + 1) / 12)
z = (U - mean_U) / std_U
r = z / np.sqrt(n1 + n2)

print(f"U = {U:.2f}, p = {p:.4f}, z = {z:.3f}, r = {r:.3f}")

"""r = 0.193 ‚Üí this is a small effect size (since 0.1 = small, 0.3 = medium).

p = .063 ‚Üí not statistically significant, but it‚Äôs close to .05 ‚Äî we‚Äôd call this a trend-level difference.

üß† Interpretation:
There‚Äôs a slight tendency for IG reels to get higher engagement than carousels, but the difference is small and not statistically reliable.
So you can say:

Although IG reels showed somewhat higher total interactions on average, the difference compared to carousels was small (r = .19) and did not reach statistical significance (U = 1014.00, p = .063).

basically: the post type has a minor effect, but it‚Äôs not strong enough to claim a real difference in engagement.

## 5 Engagement rate
"""

df_clean.head()

# how many different type of post?
df['Post type'].unique()

#group by post type
#grouped_post_type = df.groupby('Post type')

#Ë®àÁÆóÁü≠ÂΩ±ÁâáÂíåË≤ºÊñáÁöÑÁ∏Ω‰∫íÂãïÊï∏
summary = df.groupby('Post type')['Engagement_Rate'].agg(['count','mean','std','median','min','max']).reset_index()
print(summary)

# see the ditribution
import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(8,5))
sns.histplot(data=df,
             x='Engagement_Rate',
             hue='Post type',
             kde=True,
             bins=30,
             alpha=0.6)

plt.title('Distribution of Engagement_Rate (Before Removing Outliers)')
plt.xlabel('Engagement_Rate')
plt.ylabel('Count')
plt.show()

#remove outliers

# Calculate IQR for each post type separately
def remove_outliers(group):
    Q1 = group['Engagement_Rate'].quantile(0.25)
    Q3 = group['Engagement_Rate'].quantile(0.75)
    IQR = Q3 - Q1
    lower = Q1 - 1.5 * IQR
    upper = Q3 + 1.5 * IQR
    return group[(group['Engagement_Rate'] >= lower) & (group['Engagement_Rate'] <= upper)]

df_no_outliers = df.groupby('Post type', group_keys=False).apply(remove_outliers)

# check how many removed
print("Before:", len(df))
print("After:", len(df_no_outliers))

# who are the outlier?

def get_outliers(group):
    Q1 = group['Engagement_Rate'].quantile(0.25)
    Q3 = group['Engagement_Rate'].quantile(0.75)
    IQR = Q3 - Q1
    lower = Q1 - 1.5 * IQR
    upper = Q3 + 1.5 * IQR
    return group[(group['Engagement_Rate'] < lower) | (group['Engagement_Rate'] > upper)]

outliers = df.groupby('Post type', group_keys=False).apply(get_outliers)

print("Number of outliers:", len(outliers))
outliers[['Post type', 'Engagement_Rate', 'Des_eng', 'Permalink']]

#1Ô∏è‚É£ Recalculate descriptive stats
#Run your groupby summary again:

summary_clean = df_no_outliers.groupby('Post type')['Engagement_Rate'].agg(['count','mean','std','median','min','max']).reset_index()
print(summary_clean)

#distribution visual

import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(8,5))
sns.histplot(data=df_no_outliers,
             x='Engagement_Rate',
             hue='Post type',
             kde=True,
             bins=30,
             alpha=0.6)

plt.title('Distribution of Engagement_Rate (After Removing Outliers)')
plt.xlabel('Engagement_Rate')
plt.ylabel('Count')
plt.show()

#2Ô∏è‚É£ Check normality (optional but useful)
#If you plan to do statistical tests:

from scipy.stats import shapiro
for t, g in df_no_outliers.groupby('Post type'):
    print(t, shapiro(g['Engagement_Rate']))

"""IG carousel: p = 0.059 ‚Üí this is slightly above .05, so you fail to reject H‚ÇÄ ‚Üí ‚úÖ engagement rates for carousels can be considered approximately normal.

IG reel: p = 0.0057 ‚Üí this is below .05, so you reject H‚ÇÄ ‚Üí ‚ùå reels‚Äô engagement rates are not normally distributed.

üß† Summary:

Carousel ‚Üí roughly normal

Reel ‚Üí not normal

So overall, your data violate normality (since at least one group isn‚Äôt normal).
‚Üí That means you should still use a non-parametric test (Mann‚ÄìWhitney U) to compare engagement rates between post types.
"""

from scipy.stats import mannwhitneyu

# Split by post type
carousel = df_no_outliers[df_no_outliers['Post type'] == 'IG carousel']['Engagement_Rate']
reel = df_no_outliers[df_no_outliers['Post type'] == 'IG reel']['Engagement_Rate']

# Run Mann‚ÄìWhitney U test (two-tailed)
stat, p = mannwhitneyu(carousel, reel, alternative='two-sided')

print(f"U statistic = {stat:.2f}")
print(f"p-value = {p:.4f}")

"""nice üëç here‚Äôs the breakdown:

p = 0.3796 ‚Üí way above .05 ‚Üí you fail to reject H‚ÇÄ.

that means there‚Äôs no significant difference in engagement rate between IG carousels and IG reels.

U = 920.50 is the test statistic, but since p is large, it‚Äôs not meaningful here.

üß† Interpretation:
Even after accounting for outliers and non-normality, post type (carousel vs. reel) does not significantly affect engagement rate.
You can phrase it in APA style like:

A Mann‚ÄìWhitney U test showed no significant difference in engagement rate between IG carousels (Mdn = ‚Ä¶) and IG reels (Mdn = ‚Ä¶), U = 920.50, p = .38.

basically ‚Äî post type doesn‚Äôt seem to matter much for engagement rate either.
"""



"""## 6 ‰∏ÄÂÄãÂäüËÉΩÊääÁµêÊûúÁõ¥Êé•Ë∑ëÂÆå"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import shapiro, mannwhitneyu
import numpy as np

def analyze_group_diff(df, group_col, value_col):
    print(f"\nüìä Analyzing: {value_col} grouped by {group_col}\n")

    # --- 1Ô∏è‚É£ Remove outliers using IQR within each group ---
    def remove_outliers(g):
        Q1, Q3 = g[value_col].quantile([0.25, 0.75])
        IQR = Q3 - Q1
        lower, upper = Q1 - 1.5 * IQR, Q3 + 1.5 * IQR
        return g[(g[value_col] >= lower) & (g[value_col] <= upper)]

    df_no_outliers = df.groupby(group_col, group_keys=False).apply(remove_outliers)
    print(f"Original n = {len(df)}, after removing outliers = {len(df_no_outliers)}")

    # --- 2Ô∏è‚É£ Identify outliers ---
    def get_outliers(g):
        Q1, Q3 = g[value_col].quantile([0.25, 0.75])
        IQR = Q3 - Q1
        lower, upper = Q1 - 1.5 * IQR, Q3 + 1.5 * IQR
        return g[(g[value_col] < lower) | (g[value_col] > upper)]

    outliers = df.groupby(group_col, group_keys=False).apply(get_outliers)
    print("\nüö® Outliers detected:")
    if len(outliers) == 0:
        print("No outliers found.")
    else:
        display(outliers[[group_col, value_col]])

    # --- 3Ô∏è‚É£ Distribution visualization ---
    plt.figure(figsize=(8,5))
    sns.histplot(data=df_no_outliers, x=value_col, hue=group_col, kde=True, bins=30, alpha=0.6)
    plt.title(f'Distribution of {value_col} by {group_col} (After Removing Outliers)')
    plt.xlabel(value_col)
    plt.ylabel('Count')
    plt.show()

    # --- 4Ô∏è‚É£ Normality check (Shapiro-Wilk) ---
    print("\nüß™ Shapiro‚ÄìWilk Normality Test:")
    for name, g in df_no_outliers.groupby(group_col):
        stat, p = shapiro(g[value_col])
        print(f"{name}: W = {stat:.3f}, p = {p:.4f}")

    # --- 5Ô∏è‚É£ Mann‚ÄìWhitney U Test + Effect Size ---
    groups = list(df_no_outliers[group_col].unique())
    if len(groups) == 2:
        g1 = df_no_outliers[df_no_outliers[group_col] == groups[0]][value_col]
        g2 = df_no_outliers[df_no_outliers[group_col] == groups[1]][value_col]
        U, p = mannwhitneyu(g1, g2, alternative='two-sided')

        # Calculate effect size (r)
        n1, n2 = len(g1), len(g2)
        mean_U = n1 * n2 / 2
        std_U = np.sqrt(n1 * n2 * (n1 + n2 + 1) / 12)
        z = (U - mean_U) / std_U
        r = z / np.sqrt(n1 + n2)

        print(f"\n‚öñÔ∏è Mann‚ÄìWhitney U Test between {groups[0]} and {groups[1]}:")
        print(f"U = {U:.2f}, p = {p:.4f}, z = {z:.3f}, r = {r:.3f}")

        if p < 0.05:
            print("‚Üí Significant difference (p < .05)")
        else:
            print("‚Üí No significant difference (p ‚â• .05)")

        # Effect size interpretation
        if abs(r) < 0.1:
            effect = "negligible"
        elif abs(r) < 0.3:
            effect = "small"
        elif abs(r) < 0.5:
            effect = "medium"
        else:
            effect = "large"
        print(f"Effect size: {r:.3f} ({effect})")
    else:
        print("\n‚öñÔ∏è Mann‚ÄìWhitney U Test skipped (requires exactly 2 groups).")

    return df_no_outliers

analyze_group_diff(df, "Label", "Engagement_Rate")

"""### ÂΩ±ÁâáÈÇÑÊòØË≤ºÊñáÔºüË™∞ÊØîËºÉÊúÉÊúâËøΩËπ§ËÄÖ"""

analyze_group_diff(df, "Post type", "Follows")

"""### ÂΩ±ÁâáÈÇÑÊòØË≤ºÊñáÊØîËºÉÊúÉË¢´ÂàÜ‰∫´Ôºü"""

analyze_group_diff(df, "Post type", "Shares")

"""### ÂΩ±ÁâáÈÇÑÊòØË≤ºÊñáÔºåË¢´saves"""

analyze_group_diff(df, "Post type", "Saves")

"""### ÂΩ±ÁâáÈÇÑÊòØË≤ºÊñáË¢´ Comments"""

analyze_group_diff(df, "Post type", "Comments")

"""## 7 who are our audience"""

import pandas as pd
import gspread
from google.colab import auth
from google.auth import default

# 1Ô∏è‚É£ authenticate Google
auth.authenticate_user()
creds, _ = default()
gc = gspread.authorize(creds)

# 2Ô∏è‚É£ open your Google Sheet by name (title seen in Drive)
sh = gc.open("Audience")

# 3Ô∏è‚É£ list all worksheets
worksheets = sh.worksheets()
print("All pages:", [ws.title for ws in worksheets])

# 4Ô∏è‚É£ read all pages into DataFrames (UTF-8 by default)
df_dict = {}
for ws in worksheets:
    df = pd.DataFrame(ws.get_all_records())  # Google Sheets data is UTF-8 encoded automatically
    df_dict[ws.title] = df
    print(f"‚úÖ Loaded: {ws.title} ({df.shape[0]} rows)")



# Example: access a specific page
df_age_gender = df_dict['Age gender']
df_cities = df_dict['Top cities']
df_countries = df_dict['Top countries']
df_follows = df_dict['Follows']
df_top_pages = df_dict['Top pages']
# check
df_age_gender.head()

df_cities.head()

df_countries.head()

df_follows.head()

df_top_pages.head()

"""### 7-1 Age and Gender"""

import seaborn as sns
import matplotlib.pyplot as plt

# melt the data so you have 'Gender' and 'Percent' columns
m = df_age_gender.melt(id_vars='Age & gender', value_vars=['Women', 'Men'],
                       var_name='Gender', value_name='Percent')

plt.figure(figsize=(5,5))

#change color
palette = {'Women': '#f150ad', 'Men': '#f6f4de'}

ax = sns.barplot(data=m, x='Age & gender', y='Percent', hue='Gender', palette=palette)  # <-- assign to ax
plt.title('Audience by Age & Gender')
plt.xlabel('Age group')
plt.ylabel('Percent')




# remove chart frame (top & right)
sns.despine(top=True, right=True, left=True, bottom=True)

# add labels on each bar
for container in ax.containers:
    ax.bar_label(container, fmt='%.1f', label_type='edge', fontsize=8, color='black', padding=2)



plt.show()

"""### set up to read chinese

Ëß£Ê≥ï‰æÜËá™Ôºöhttps://colab.research.google.com/github/willismax/matplotlib_show_chinese_in_colab/blob/master/matplotlib_show_chinese_in_colab.ipynb#scrollTo=mzmPaN_xHfQg
"""

# Colab ÈÄ≤Ë°åmatplotlibÁπ™ÂúñÊôÇÈ°ØÁ§∫ÁπÅÈ´î‰∏≠Êñá
# ‰∏ãËºâÂè∞ÂåóÊÄùÊ∫êÈªëÈ´î‰∏¶ÂëΩÂêçtaipei_sans_tc_beta.ttfÔºåÁßªËá≥ÊåáÂÆöË∑ØÂæë
!wget -O TaipeiSansTCBeta-Regular.ttf https://drive.google.com/uc?id=1eGAsTN1HBpJAkeVM57_C7ccp7hbgSz3_&export=download

import matplotlib

# ÊîπstyleË¶ÅÂú®Êîπfont‰πãÂâç
# plt.style.use('seaborn')

matplotlib.font_manager.fontManager.addfont('TaipeiSansTCBeta-Regular.ttf')
matplotlib.rc('font', family='Taipei Sans TC Beta')

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

"""### 7-2 Top pages what are they watching? if not us?"""

df_top_pages.head()

df_top_pages.columns

import seaborn as sns
import matplotlib.pyplot as plt

# melt into long format
df_top_pages_melt = df_top_pages.melt(var_name='Page', value_name='Percent')

# sort by Percent descending
df_top_pages_melt = df_top_pages_melt.sort_values('Percent', ascending=False)



plt.figure(figsize=(5,5))
ax = sns.barplot(data=df_top_pages_melt, y='Page', x='Percent')  # you can change color here

plt.title('Top Pages Followed by Audience')
plt.xlabel('Percent (%)')
plt.ylabel('')
sns.despine(top=True, right=True)

# add labels on each bar
for container in ax.containers:
    ax.bar_label(container, fmt='%.1f', label_type='edge', fontsize=8, color='black', padding=2)


plt.show()

import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(5,5))
ax = sns.barplot(
    data=df_top_pages_melt,
    y='Page',
    x='Percent',
    color='#92ba92'
)

plt.title('Top Pages Followed by Audience', fontsize=12)
plt.xlabel('Percent (%)')
plt.ylabel('')
sns.despine(top=True, right=True, bottom=True, left=True)

# remove y-axis labels since we'll draw them on bars
ax.set_yticklabels([])

# draw each category name on its bar (Â∑¶Â∞çÈΩä„ÄÅÊîæÂú® bar ÂÖß‰∏äÊñπ)
for p, label in zip(ax.patches, df_top_pages_melt['Page']):
    ax.text(
        0.3,                                 # slight right padding from left edge
        p.get_y() + p.get_height() * 0.7,    # a bit above bar middle
        label,
        ha='left', va='center', color='white', fontsize=9, weight='bold'
    )
    # numeric label on right
    ax.text(
        p.get_width() + 0.3,
        p.get_y() + p.get_height() / 2,
        f"{p.get_width():.1f}",
        ha='left', va='center', color='black', fontsize=9
    )

plt.tight_layout()
plt.show()

"""# 8 Which topic is most popular (engagment rate)"""

df_clean.head()

import matplotlib.pyplot as plt

# mapping English labels to Chinese names
label_map = {
    'A1': 'ÊäïË≥áÁêÜË≤°Á≠ñÁï•',
    'A2': 'Ë¶™Â≠êË≤°ÂïÜÊïôËÇ≤',
    'A3': 'ÈÄÄ‰ºëË¶èÂäÉ',
    'A4': '‰øùÈö™Ë¶èÂäÉ',
    'A5': 'ÂÄã‰∫∫ÊàêÈï∑ & ÁêÜË≤°ËßÄÂøµ',
    'A6': 'Á®ÖÂãô & ÂÖ¨Âè∏Ë≤°Âãô',
    'A7': 'Áî¢Ê•≠Ë∂®Âã¢ & ÊôÇ‰∫ãË©ïË´ñ',
    'A8': 'Â•≥ÊÄßÁêÜË≤° & ÁîüÊ¥ªÂì≤Â≠∏'
}

# group by label
topic_engagement = (
    df_clean.groupby('Label')['Engagement_Rate']
    .mean()
    .sort_values(ascending=False)
    .reset_index()
)

# add Chinese names
topic_engagement['Label_Chinese'] = topic_engagement['Label'].map(label_map)

# visualize
plt.figure(figsize=(8,5))
plt.barh(topic_engagement['Label_Chinese'], topic_engagement['Engagement_Rate'])
plt.xlabel('Âπ≥Âùá‰∫íÂãïÁéá (Engagement Rate)')
plt.ylabel('‰∏ªÈ°å (Topic)')
plt.title('ÂêÑ‰∏ªÈ°åÂπ≥Âùá‰∫íÂãïÁéá')
plt.gca().invert_yaxis()
plt.show()

print(topic_engagement)

import matplotlib.pyplot as plt
import numpy as np

# mapping English labels to Chinese names
label_map = {
    'A1': 'ÊäïË≥áÁêÜË≤°Á≠ñÁï•',
    'A2': 'Ë¶™Â≠êË≤°ÂïÜÊïôËÇ≤',
    'A3': 'ÈÄÄ‰ºëË¶èÂäÉ',
    'A4': '‰øùÈö™Ë¶èÂäÉ',
    'A5': 'ÂÄã‰∫∫ÊàêÈï∑ & ÁêÜË≤°ËßÄÂøµ',
    'A6': 'Á®ÖÂãô & ÂÖ¨Âè∏Ë≤°Âãô',
    'A7': 'Áî¢Ê•≠Ë∂®Âã¢ & ÊôÇ‰∫ãË©ïË´ñ',
    'A8': 'Â•≥ÊÄßÁêÜË≤° & ÁîüÊ¥ªÂì≤Â≠∏'
}

# group and sort
topic_engagement = (
    df_clean.groupby('Label')['Engagement_Rate']
    .mean()
    .sort_values(ascending=False)
    .reset_index()
)

# add Chinese names
topic_engagement['Label_Chinese'] = topic_engagement['Label'].map(label_map)

# gradient color (yellow hue)
colors = plt.cm.YlOrBr(np.linspace(0.4, 1, len(topic_engagement)))

# plot
plt.figure(figsize=(8, 5))
bars = plt.barh(topic_engagement['Label_Chinese'], topic_engagement['Engagement_Rate'], color=colors)

# add labels beside bars
for bar in bars:
    plt.text(
        bar.get_width() + 0.002,  # slightly to the right of bar
        bar.get_y() + bar.get_height()/2,
        f"{bar.get_width():.2f}",  # format to 2 decimals
        va='center',
        fontsize=10
    )

plt.xlabel('Âπ≥Âùá‰∫íÂãïÁéá (Engagement Rate)')
plt.ylabel('‰∏ªÈ°å (Topic)')
plt.title('ÂêÑ‰∏ªÈ°åÂπ≥Âùá‰∫íÂãïÁéá')
plt.gca().invert_yaxis()
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import numpy as np

# label map
label_map = {
    'A1': 'ÊäïË≥áÁêÜË≤°Á≠ñÁï•',
    'A2': 'Ë¶™Â≠êË≤°ÂïÜÊïôËÇ≤',
    'A3': 'ÈÄÄ‰ºëË¶èÂäÉ',
    'A4': '‰øùÈö™Ë¶èÂäÉ',
    'A5': 'ÂÄã‰∫∫ÊàêÈï∑ & ÁêÜË≤°ËßÄÂøµ',
    'A6': 'Á®ÖÂãô & ÂÖ¨Âè∏Ë≤°Âãô',
    'A7': 'Áî¢Ê•≠Ë∂®Âã¢ & ÊôÇ‰∫ãË©ïË´ñ',
    'A8': 'Â•≥ÊÄßÁêÜË≤° & ÁîüÊ¥ªÂì≤Â≠∏'
}

# count posts per label
label_count = df_clean['Label'].value_counts().sort_values(ascending=False)
label_count = label_count.rename_axis('Label').reset_index(name='Count')
label_count['Label_Chinese'] = label_count['Label'].map(label_map)

# gradient color (yellow hue)
colors = plt.cm.YlOrBr(np.linspace(0.4, 1, len(label_count)))

# plot
plt.figure(figsize=(8,5))
bars = plt.bar(label_count['Label_Chinese'], label_count['Count'], color=colors)

# add count on top
for bar in bars:
    plt.text(bar.get_x() + bar.get_width()/2,
             bar.get_height() + 0.2,
             f"{int(bar.get_height())}",
             ha='center', va='bottom', fontsize=10)

plt.xlabel('‰∏ªÈ°å (Topic)')
plt.ylabel('Ë≤ºÊñáÊï∏Èáè (Post Count)')
plt.title('ÂêÑ‰∏ªÈ°åË≤ºÊñáÊï∏Èáè')
plt.xticks(rotation=25, ha='right')
plt.tight_layout()
plt.show()

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

label_map = {
    'A1': 'ÊäïË≥áÁêÜË≤°Á≠ñÁï•','A2': 'Ë¶™Â≠êË≤°ÂïÜÊïôËÇ≤','A3': 'ÈÄÄ‰ºëË¶èÂäÉ','A4': '‰øùÈö™Ë¶èÂäÉ',
    'A5': 'ÂÄã‰∫∫ÊàêÈï∑ & ÁêÜË≤°ËßÄÂøµ','A6': 'Á®ÖÂãô & ÂÖ¨Âè∏Ë≤°Âãô','A7': 'Áî¢Ê•≠Ë∂®Âã¢ & ÊôÇ‰∫ãË©ïË´ñ','A8': 'Â•≥ÊÄßÁêÜË≤° & ÁîüÊ¥ªÂì≤Â≠∏'
}

# ÂÖºÂÆπÊ¨Ñ‰ΩçÂêçÔºöEngagement_Rate Êàñ Avg_eng
eng_col = 'Engagement_Rate' if 'Engagement_Rate' in df_clean.columns else 'Avg_eng'

summary = (
    df_clean.groupby('Label')
            .agg(Engagement_Rate=(eng_col, 'mean'),
                 Post_Count=('Label', 'count'))
            .reset_index()
            .sort_values('Engagement_Rate', ascending=False)
)
summary['Label_Chinese'] = summary['Label'].map(label_map)

fig, ax1 = plt.subplots(figsize=(9,5))

# Bar: ‰∫íÂãïÁéá
colors = plt.cm.YlOrBr(np.linspace(0.4, 1, len(summary)))
bars = ax1.bar(summary['Label_Chinese'], summary['Engagement_Rate'],
               color=colors, label='Âπ≥Âùá‰∫íÂãïÁéá (Bar)')
for b in bars:
    ax1.text(b.get_x()+b.get_width()/2, b.get_height()+0.002,
             f"{b.get_height():.2f}", ha='center', va='bottom', fontsize=9)

ax1.set_ylabel('Âπ≥Âùá‰∫íÂãïÁéá', color='saddlebrown')
ax1.tick_params(axis='y', labelcolor='saddlebrown')
ax1.set_title('ÂêÑ‰∏ªÈ°åÊäïÂÖ•ËàáË°®ÁèæÔºàBar=‰∫íÂãïÁéáÔºåLine=Ë≤ºÊñáÊï∏Ôºâ')

# Line: Ë≤ºÊñáÊï∏
ax2 = ax1.twinx()
(line,) = ax2.plot(summary['Label_Chinese'], summary['Post_Count'],
                   marker='o', linewidth=2, color='darkorange',
                   label='Ë≤ºÊñáÊï∏Èáè (Line)')
ax2.set_ylabel('Ë≤ºÊñáÊï∏Èáè', color='darkorange')
ax2.tick_params(axis='y', labelcolor='darkorange')

# x Ëª∏Ê®ôÁ±§ËßíÂ∫¶ÔºàÈÅøÂÖçÈáçÁñäÔºâ
for lbl in ax1.get_xticklabels():
    lbl.set_rotation(25)
    lbl.set_ha('right')

# Âêà‰ΩµÈõôËª∏Âúñ‰æã
h1, l1 = ax1.get_legend_handles_labels()
h2, l2 = ax2.get_legend_handles_labels()
ax1.legend(h1+h2, l1+l2, loc='upper left', bbox_to_anchor=(0.5, 0.9), frameon=False)




plt.tight_layout()
plt.show()

"""# 9 Word Cloud

"""

import jieba
import pandas as pd

# --- stopwords (basic only, not domain-specific yet) ---
stopwords = {
    "ÁöÑ","‰∫Ü","Âíå","ÊòØ","Âú®","Â∞±","ÈÉΩ","ËÄå","Âèä","Ëàá","Ëëó","Êàñ","‰∏ÄÂÄã","Ê≤íÊúâ",
    "ÊàëÂÄë","‰Ω†ÂÄë","‰ªñÂÄë","Ëá™Â∑±","ÈÄôÂÄã","ÈÇ£ÂÄã","Â¶ÇÊûú","ÂèØ‰ª•","Âõ†ÁÇ∫","ÊâÄ‰ª•",
    "ËÆì","Âéª","‰æÜ","ÊúÉ","Ë¶Å","Êää","‰πü","Âæà","ÈÇÑ","Êõ¥","‰ª•Âèä","‰ΩÜ","Âë¢","Âïä",
    "ÁêÜË≤°","ÁêÜË≤°Â∞èÊôÇÂÄô","ÊäïË≥á","ÊäïË≥áÁêÜË≤°","Ë≤°ÂïÜÊïôËÇ≤","ËÇ°Á•®","ËÇ°Á•®ÊäïË≥á","ÊäïË≥áËÇ°Á•®",
    "https", "moneywings", "Áü•ÈÅì","ÂÖ∂ÂØ¶","ËÄåÊòØ","ÂæàÂ§ö","com", "org", "instagram",
    "IG", "firstory", "io", "ft", "ÈÅ©Âêà", "Â∞èÂßêÂßê", "www", "ÈÇÑÊòØ", "‰∏çÊòØ", "ÁúüÊ≠£",
    "podcast"
}

# --- tokenize and count words by label ---
word_counts_by_label = {}

for label in df_clean["Label"].unique():
    text = " ".join(df_clean[df_clean["Label"] == label]["Description"].astype(str))
    words = [
        w for w in jieba.cut(text)
        if w.strip() and w not in stopwords and len(w) > 1
    ]
    word_freq = pd.Series(words).value_counts().reset_index()
    word_freq.columns = ["Word", "Count"]
    word_counts_by_label[label] = word_freq

# --- example: view top 30 words for label A1 ---
print("Top 30 words for A1:")
print(word_counts_by_label["A1"].head(30))

from wordcloud import WordCloud
import matplotlib.pyplot as plt
import jieba

# --- use your custom font ---
import matplotlib
matplotlib.font_manager.fontManager.addfont('TaipeiSansTCBeta-Regular.ttf')
matplotlib.rc('font', family='Taipei Sans TC Beta')

font_path = 'TaipeiSansTCBeta-Regular.ttf'

# --- stopwords ---
stopwords = {
     "ÁöÑ","‰∫Ü","Âíå","ÊòØ","Âú®","Â∞±","ÈÉΩ","ËÄå","Âèä","Ëàá","Ëëó","Êàñ","‰∏ÄÂÄã","Ê≤íÊúâ",
    "ÊàëÂÄë","‰Ω†ÂÄë","‰ªñÂÄë","Ëá™Â∑±","ÈÄôÂÄã","ÈÇ£ÂÄã","Â¶ÇÊûú","ÂèØ‰ª•","Âõ†ÁÇ∫","ÊâÄ‰ª•",
    "ËÆì","Âéª","‰æÜ","ÊúÉ","Ë¶Å","Êää","‰πü","Âæà","ÈÇÑ","Êõ¥","‰ª•Âèä","‰ΩÜ","Âë¢","Âïä",
    "ÁêÜË≤°","ÁêÜË≤°Â∞èÊôÇÂÄô","ÊäïË≥á","ÊäïË≥áÁêÜË≤°","Ë≤°ÂïÜÊïôËÇ≤","ËÇ°Á•®","ËÇ°Á•®ÊäïË≥á","ÊäïË≥áËÇ°Á•®",
    "https", "moneywings", "Áü•ÈÅì","ÂÖ∂ÂØ¶","ËÄåÊòØ","ÂæàÂ§ö","com", "org", "instagram",
    "IG", "firstory", "io", "ft", "ÈÅ©Âêà", "Â∞èÂßêÂßê", "www", "ÈÇÑÊòØ", "‰∏çÊòØ", "ÁúüÊ≠£",
    "podcast", "ÁÑ°Ê≥ï", "ÂøÖÁúã", "ÂÆÉÂÄë", "Ë©πÁíá", "Â∞±ÊòØ", "‰ªÄÈ∫º", "Âè™Ë¶Å", "‰∏ÄÂÆö", "‰ªÄÈ∫º",
     "ÊÄéÈ∫º", "Âè™ÊòØ", "ÁúüÁöÑ", "SHINLI", "Â§öÂ∞ë", "ÈáçÈªû", "Â§©‰∏ã"
}

# --- function to generate word cloud for each label ---
def make_wordcloud_by_label(df, label_name):
    text = " ".join(df[df["Label"] == label_name]["Description"].astype(str))
    words = [w for w in jieba.cut(text) if w.strip() and w not in stopwords and len(w) > 1]

    wc = WordCloud(
        font_path=font_path,
        width=800, height=600,
        background_color="white",
        colormap="OrRd",
        max_words=15
    ).generate(" ".join(words))

    plt.figure(figsize=(8,6))
    plt.imshow(wc, interpolation="bilinear")
    plt.axis("off")
    plt.title(f"Label {label_name} - Word Cloud", fontsize=14)
    plt.show()

# --- generate for each label ---
for label in df_clean["Label"].unique():
    make_wordcloud_by_label(df_clean, label)

"""# 10 Average not raw count"""

df_clean.columns

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np

# make sure names are clean
df_clean['Post type'] = df_clean['Post type'].str.strip()

# color palette (same for all charts)
palette = {'IG reel': '#FFB6B9', 'IG carousel': '#8AC6D1'}
order = ['IG reel', 'IG carousel']

# 1Ô∏è‚É£ Average Engagement Rate per Post Type
avg_eng = df_clean.groupby('Post type')['Engagement_Rate'].mean().reset_index()

plt.figure(figsize=(6,4))
sns.barplot(
    data=avg_eng, x='Post type', y='Engagement_Rate',
    palette=palette, order=order
)
plt.title('Average Engagement Rate by Post Type')
plt.xlabel('')
plt.ylabel('Average Engagement Rate')
plt.tight_layout()
plt.show()

# 2Ô∏è‚É£ Distribution (all data)
plt.figure(figsize=(6,4))
sns.boxplot(
    data=df_clean, x='Post type', y='Engagement_Rate',
    palette=palette, order=order
)
plt.title('Distribution of Engagement Rate by Post Type')
plt.xlabel('')
plt.ylabel('Engagement Rate')
plt.tight_layout()
plt.show()

# 3Ô∏è‚É£ Equal sample size (fair comparison)
min_n = df_clean['Post type'].value_counts().min()
df_sampled = df_clean.groupby('Post type').sample(n=min_n, random_state=42)

plt.figure(figsize=(6,4))
sns.boxplot(
    data=df_sampled, x='Post type', y='Engagement_Rate',
    palette=palette, order=order
)
plt.title('Fair Comparison (Equal Sample Size)')
plt.xlabel('')
plt.ylabel('Engagement Rate')
plt.tight_layout()
plt.show()

"""```
# This is formatted as code
```

# 1Ô∏è‚É£ ‰∏çÂêåÂàÜÈ°ûÁöÑÂπ≥Âùá‰∫íÂãïÁéáÊØîËºÉÔºàÊ©´Ê¢ùÂúñÔºâ

## Data Cleaning
"""

import pandas as pd
import matplotlib.pyplot as plt
import matplotlib
from matplotlib import font_manager

# Ë®≠ÂÆöÂ≠óÈ´î
plt.rcParams['font.family'] = 'sans-serif'
plt.rcParams['axes.unicode_minus'] = False

# ËÆÄÂèñË≥áÊñô
# Ë´ãÂ∞á 'your_data.csv' ÊõøÊèõÊàê‰Ω†ÁöÑÂØ¶ÈöõÊ™îÊ°àË∑ØÂæë
df = pd.read_csv('/content/drive/MyDrive/data/data.csv')

# keep only the part before '-'
#df['ÂàÜÈ°ûÁ∑®Á¢º'] = df['ÂàÜÈ°ûÁ∑®Á¢º'].str.split('-').str[0]

# confirm change
#print(df['ÂàÜÈ°ûÁ∑®Á¢º'].unique())


# Ë≥áÊñôÊ∏ÖÁêÜÔºöÁ¢∫‰øùÊï∏ÂÄºÊ¨Ñ‰ΩçÊòØÊï∏Â≠óÊ†ºÂºè Like+Comment+Saves+Shares+follows
df['Likes'] = pd.to_numeric(df['Likes'], errors='coerce')
df['Comments'] = pd.to_numeric(df['Comments'], errors='coerce')
df['Saves'] = pd.to_numeric(df['Saves'], errors='coerce')
df['Reach'] = pd.to_numeric(df['Reach'], errors='coerce')

#sares + follows
df['Shares'] = pd.to_numeric(df['Shares'], errors='coerce')
df['Follows'] = pd.to_numeric(df['Follows'], errors='coerce')


# Ë®àÁÆó‰∫íÂãïÁéá
df['Engagement_Rate'] = (df['Likes'] + df['Comments'] + df['Saves']+ df['Shares'] + df['Follows']) / df['Reach'] * 100

# ÊåâÂàÜÈ°ûË®àÁÆóÂπ≥Âùá‰∫íÂãïÁéá
category_engagement = df.groupby('ÂàÜÈ°ûÁ∑®Á¢º')['Engagement_Rate'].mean().sort_values()

# ÂÆöÁæ©ÂàÜÈ°ûÂêçÁ®±Â∞çÁÖßÔºàËã±ÊñáÁâàÔºâ
category_names = {
    'A1': 'Investment Strategy',
    'A2': 'Kids Finance Edu',
    'A3': 'Retirement Plan',
    'A4': 'Insurance',
    'A5': 'Personal Growth',
    'A6': 'Tax & Business',
    'A7': 'Industry Trends',
    'A8': 'Women Finance'
}

# ÊõøÊèõÂàÜÈ°ûÂêçÁ®±
category_engagement.index = category_engagement.index.map(category_names)

"""## Visualization"""

# Âª∫Á´ãÂúñË°®
fig, ax = plt.subplots(figsize=(8, 8))  # ‚úÖ 1:1 ÊØî‰æãÔºåÈÅ©Âêà IG Ë≤ºÊñá
ax.set_aspect('auto')  # Ëá™ÂãïË™øÊï¥Èï∑Ê¢ùÊØî‰æã
plt.tight_layout(pad=3)  # ‰øùÁïôÈÇäÁïåÁ©∫ÈñìÈÅøÂÖçË¢´Ë£ÅÊéâ


# Áπ™Ë£ΩÊ©´Ê¢ùÂúñ


# Â•óÁî®È°èËâ≤Âà∞ bar
#colors = [color_map.get(cat, '#FF6B9D') for cat in category_engagement.index]

bars = ax.barh(
    category_engagement.index,
    category_engagement.values,
    color=colors,
    #edgecolor='#C7254E',
    linewidth=1.5
)


# Âú®ÊØèÂÄã bar ‰∏äÈ°ØÁ§∫„Äå‰πæÊ∑®Áâà„ÄçÂàÜÈ°ûÂêçÁ®±
for i, bar in enumerate(bars):
    cat = category_engagement.index[i]
    label = clean_category_names.get(cat, cat)
    y = bar.get_y() + bar.get_height() / 2
    x = bar.get_width() * 0.02  # ËÆìÊñáÂ≠óÁ®çÂæÆÈù†Â∑¶‰∏ÄÈªû
    ax.text(x, y, label, va='center', ha='left', fontsize=8, color='#333333', fontweight='bold')




# ‰øÆÊ≠£ÔºöÈáùÂ∞ç„ÄåÊ∞¥Âπ≥Èï∑Ê¢ùÂúñ barh„ÄçÁöÑÊï∏ÂÄºÊ®ôË®ªÔºåÈÅøÂÖçÊì†Âú®‰∏ÄËµ∑
xmax = float(category_engagement.max())
ax.set_xlim(0, xmax * 1.15)  # È†êÁïôÂè≥ÂÅ¥Á©∫ÈñìÁµ¶Ê®ôÁ±§
pad = xmax * 0.015           # ‰æùË≥áÊñôËá™ÂãïË™øÊï¥ÂÖßÂ§ñË∑ù

for bar in bars:
    w = bar.get_width()
    y = bar.get_y() + bar.get_height() / 2
    # ÂØ¨ÁöÑ bar ÂÖßÂµåÁôΩÂ≠óÔºõÁ™ÑÁöÑ bar ÊîæÂ§ñÂÅ¥Ê∑±Ëâ≤Â≠óÔºåÈÅøÂÖçÈáçÁñä
    if w > xmax * 0.12:
        ax.text(w - pad, y, f'{w:.2f}%', ha='right', va='center',
                fontsize=9, fontweight='bold', color='white')
    else:
        ax.text(w + pad, y, f'{w:.2f}%', ha='left', va='center',
                fontsize=9, fontweight='bold', color='#333333')



# Ë®≠ÂÆöÊ®ôÈ°åÂíåÊ®ôÁ±§
ax.set_xlabel('Average Engagement Rate (%)', fontsize=12, fontweight='bold')
ax.set_ylabel('Content Category', fontsize=8, fontweight='bold')
ax.set_title('Average Engagement Rate by Content Category\nEngagement Rate = (Likes + Comments + Saves + Shares + Follows) / Reach',
             fontsize=14, fontweight='bold', pad=20)

# ÁæéÂåñÂúñË°®
ax.grid(axis='x', alpha=0.3, linestyle='--')
ax.set_axisbelow(True)
plt.tight_layout()


# È°ØÁ§∫ÂúñË°®
plt.show()

# Ëº∏Âá∫Áµ±Ë®àË≥áË®ä
print("=" * 60)
print("üìä Engagement Rate Statistics by Category")
print("=" * 60)
for category, rate in category_engagement.items():
    print(f"{category}: {rate:.2f}%")
print("=" * 60)
print(f"\nüèÜ Highest Engagement: {category_engagement.idxmax()} ({category_engagement.max():.2f}%)")
print(f"üìâ Lowest Engagement: {category_engagement.idxmin()} ({category_engagement.min():.2f}%)")
print(f"üìä Average Engagement: {category_engagement.mean():.2f}%")

import matplotlib.pyplot as plt
from matplotlib.patches import Patch

# ‰øùË≠âÈ†ÜÂ∫èÔºàËã•‰Ω†ÊÉ≥Âõ∫ÂÆö A1‚ÜíA8Ôºâ
order = ['A1','A2','A3','A4','A5','A6','A7','A8']
category_engagement = category_engagement.reindex([c for c in order if c in category_engagement.index]).sort_values(ascending=True)

color_map = {
    'A2': '#92ba92','A4': '#a2bf95','A8': '#b1c499','A5': '#bfc99e',
    'A3': '#cdcda5','A1': '#dad2ad','A6': '#e6d8b5','A7': '#f1ddbf'
}
full_names = {
    'A1': 'Investment Strategy','A2': 'Kids Finance Edu','A3': 'Retirement Plan','A4': 'Insurance',
    'A5': 'Personal Growth','A6': 'Tax & Business','A7': 'Industry Trends','A8': 'Women Finance'
}

# --- Âª∫Á´ãÂúñË°® ---
fig, ax = plt.subplots(figsize=(8, 8))
ax.set_aspect('auto')
plt.tight_layout(pad=3)

# È°èËâ≤
colors = [color_map.get(cat, '#BBBBBB') for cat in category_engagement.index]

# Ê∞¥Âπ≥Èï∑Ê¢ùÂúñÔºàY Ëª∏=‰ª£Á¢º A1~A8Ôºâ
bars = ax.barh(
    category_engagement.index,   # Áõ¥Êé•Áî®‰ª£Á¢ºÂÅö Y-axis tick
    category_engagement.values,
    color=colors,
    linewidth=1.5
)

# Êï∏ÂÄºÊ®ôË®ªÔºà%Ôºâ
xmax = float(category_engagement.max())
ax.set_xlim(0, xmax * 1.15)
pad = xmax * 0.015
for bar in bars:
    w = bar.get_width()
    y = bar.get_y() + bar.get_height() / 2
    if w > xmax * 0.12:
        ax.text(w - pad, y, f'{w:.2f}%', ha='right', va='center',
                fontsize=9, fontweight='bold', color='white')
    else:
        ax.text(w + pad, y, f'{w:.2f}%', ha='left', va='center',
                fontsize=9, fontweight='bold', color='#333333')

# Ê®ôÈ°åËàáÂ∫ßÊ®ôËª∏ÔºàY Ëª∏Ê®ôÁ±§= A1‚ÄìA8Ôºâ
ax.set_xlabel('Average Engagement Rate (%)', fontsize=12, fontweight='bold')
ax.set_ylabel('Category (A1‚ÄìA8)', fontsize=10, fontweight='bold')
ax.set_title('Average Engagement Rate by Content Category\n'
             'Engagement Rate = (Likes + Comments + Saves + Shares + Follows) / Reach',
             fontsize=14, fontweight='bold', pad=20)
ax.grid(axis='x', alpha=0.3, linestyle='--')
ax.set_axisbelow(True)

# Legend Áî®„ÄåÂÆåÊï¥ÂêçÁ®±„Äç
legend_handles = [
    Patch(facecolor=color_map[k], label=full_names[k])
    for k in [c for c in order if c in category_engagement.index]
]
leg = ax.legend(handles=legend_handles, title='Category Legend',
                loc='lower right', bbox_to_anchor=(1.0, 0.0),
                fontsize=9, title_fontsize=10, frameon=False, ncol=1)

plt.tight_layout()
plt.show()

# Ëº∏Âá∫Áµ±Ë®àË≥áË®ä
print("=" * 60)
print("üìä Engagement Rate Statistics by Category")
print("=" * 60)
for category, rate in category_engagement.items():
    print(f"{category}: {rate:.2f}%")
print("=" * 60)
print(f"\nüèÜ Highest Engagement: {category_engagement.idxmax()} ({category_engagement.max():.2f}%)")
print(f"üìâ Lowest Engagement: {category_engagement.idxmin()} ({category_engagement.min():.2f}%)")
print(f"üìä Average Engagement: {category_engagement.mean():.2f}%")



import matplotlib.pyplot as plt

# === Inputs assumed ===
# category_engagement: pd.Series with index like ['A1','A2',...] and values as percentages
# category_names: dict mapping code -> 'A1-Investment Strategy' (optional; used for pretty labels)

# New color logic (code -> hex)
color_map = {
    'A2': '#92ba92',
    'A4': '#a2bf95',
    'A8': '#b1c499',
    'A5': '#bfc99e',
    'A3': '#cdcda5',
    'A1': '#dad2ad',
    'A6': '#e6d8b5',
    'A7': '#f1ddbf'
}

# Sort (small -> big) for horizontal bars
category_engagement = category_engagement.sort_values(ascending=True)

# Colors for each bar (fallback to a neutral tone if missing)
colors = [color_map.get(cat, '#CCCCCC') for cat in category_engagement.index]

# Build figure
fig, ax = plt.subplots(figsize=(8, 8))  # square-ish for IG
ax.set_aspect('auto')
plt.tight_layout(pad=3)

# Plot
bars = ax.barh(
    category_engagement.index,
    category_engagement.values,
    color=colors,
    linewidth=1.5
)

# Pretty names without code prefix (safe even if category_names not provided)
if 'category_names' in globals() and isinstance(category_names, dict):
    clean_category_names = {k: (v.split('-', 1)[1] if '-' in v else v) for k, v in category_names.items()}
else:
    clean_category_names = {code: code for code in category_engagement.index}  # fallback

# Text labels on bars (left-side category name)
for i, bar in enumerate(bars):
    cat = category_engagement.index[i]
    label = clean_category_names.get(cat, cat)
    y = bar.get_y() + bar.get_height() / 2
    x = bar.get_width() * 0.02
    ax.text(x, y, label, va='center', ha='left', fontsize=8, color='#333333', fontweight='bold')

# Value labels (inside if wide, outside if narrow)
xmax = float(category_engagement.max())
ax.set_xlim(0, xmax * 1.15)  # room on the right
pad = xmax * 0.015

for bar in bars:
    w = bar.get_width()
    y = bar.get_y() + bar.get_height() / 2
    if w > xmax * 0.12:
        ax.text(w - pad, y, f'{w:.2f}%', ha='right', va='center',
                fontsize=9, fontweight='bold', color='white')
    else:
        ax.text(w + pad, y, f'{w:.2f}%', ha='left', va='center',
                fontsize=9, fontweight='bold', color='#333333')

# Titles & axes
ax.set_xlabel('Average Engagement Rate (%)', fontsize=12, fontweight='bold')
ax.set_ylabel('Content Category', fontsize=8, fontweight='bold')
ax.set_title('Average Engagement Rate by Content Category\n'
             'Engagement Rate = (Likes + Comments + Saves + Shares + Follows) / Reach',
             fontsize=14, fontweight='bold', pad=20)

# Style
ax.grid(axis='x', alpha=0.3, linestyle='--')
ax.set_axisbelow(True)
plt.tight_layout()
plt.show()